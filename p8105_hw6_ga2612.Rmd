---
title: "p8105_hw6_ga2612"
author: "Lupe Antonio"
date: "12/1/2023"
output:
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(broom)
library(purrr)
library(modelr)
library(ggplot2)
```

## Problem 1

```{r, message = FALSE}
#loading data
homicide_data <- read_csv('data_hw6/homicide_data.csv') %>%
  mutate(
    city_state = str_c(city, state, sep = ', '),
    #unsolved == 0, solved == 1
    resolution = case_when(
      disposition == 'Closed without arrest' ~ 0,
      disposition == 'Open/No arrest' ~ 0,
      disposition == 'Closed by arrest' ~ 1),
    #making victime_age numeric
    victim_age = as.numeric(victim_age)) %>%
  #excluding cities
  filter(city_state != 'Dallas, TX',
         city_state != 'Phoenix, AZ',
         city_state != 'Kansas City, MO',
         city_state != 'Tulsa, AL',
        #filtering victim_race
        victim_race == 'White' | victim_race == 'Black')
```


```{r, message = FALSE}
baltimore_data <- homicide_data %>%
  filter(city_state == 'Baltimore, MD')

#fitting glm model
baltimore_logistic <- baltimore_data %>%
  glm(resolution ~ victim_age + victim_sex + victim_race,
    data = ., family = binomial())


#obtaining estimate
baltimore_logistic %>%
  tidy() %>%
  filter(term == 'victim_sexMale') %>%
  mutate(adj_OR = exp(estimate),
         OR_low = exp(estimate - 1.96 * std.error),
         OR_upper = exp(estimate + 1.96 * std.error)) %>%
  select(term, adj_OR, OR_low, OR_upper)

#obtaining confidence interval for checking purposes
exp(confint(baltimore_logistic)['victim_sexMale', ])
```


```{r}
cities_data <- homicide_data %>%
  nest(data = -city_state) %>%
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race,
                                 data = df, family = binomial())),
    tidy_models = map(models, tidy)) %>%
  select(-models, data) %>%
  unnest(cols = tidy_models) %>%
  filter(term == 'victim_sexMale') %>%
  mutate(adj_OR = exp(estimate),
         OR_low = exp(estimate - 1.96*std.error),
         OR_upper = exp(estimate + 1.96*std.error)) %>%
  select(city_state, term, adj_OR, OR_low, OR_upper)

cities_data %>%
  knitr::kable()
```


```{r}
cities_data %>%
  mutate(city_state = fct_reorder(city_state, adj_OR)) %>%
  ggplot(aes(x = city_state, y = adj_OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_low, ymax = OR_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

From the table above, we can see that the majority of cities have adjusted odds ratio less than 1. This suggest that among crimes against Males they have lower odds of resolution compared to crimes against Females, after controlling for the victims age and race. 



## Problem 2

```{r}
#loading data
weather_df <- 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
#fitting SLR model
model_p2 <- lm(tmax ~ tmin + prcp, data = weather_df)
```

```{r}
#bootstrapping
bootstrap_results <- weather_df %>%
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    estimates = map(models, tidy),
    results = map(models, glance)) %>%
  select(-strap, -models) %>%
  unnest(estimates, results) %>%
  select(term, estimate, r.squared) %>%
  filter(term == 'tmin' | term == 'prcp') %>%
  pivot_wider(names_from = 'term',
              values_from = 'estimate') %>%
  mutate(log_estimates = log(tmin * prcp))
```

```{r}
#plotting log estimates
bootstrap_results %>%
  ggplot(aes(x = log_estimates)) +
  geom_density()
```
Based on the plot above, we can see that the majority of samples produced by bootstrapping are concentrated around similar values, somewhere between -6 and -3. This is further supported by the confidence intervals below. 


```{r}
#plotting for r-squared
bootstrap_results %>%
  ggplot(aes(x = r.squared)) +
  geom_density()
```
Based on the plot above, we can see that the r-squared values produced by bootstrapping have a small spread as well. The confidence interval is also small. This is consistent with values/estimates becoming fairly similar due bootstrapping becoming closer to normal distribution. 


```{r}
#confidence interval
bootstrap_results %>%
  summarize(
    ci_lower_r = quantile(r.squared, 0.025, na.rm = T),
    ci_upper_r = quantile(r.squared, 0.975, na.rm = T),
    
    ci_lower_logestimates = quantile(log_estimates, 0.025, na.rm = T),
    ci_upper_logestimates = quantile(log_estimates, 0.975, na.rm = T))
```


## Problem 3

```{r}
birthweight <- read_csv('data_hw6/birthweight.csv') %>%
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace))
```

Describe your modeling process 

Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.


```{r}
#no missing data
birthweight[!complete.cases(birthweight), ]
```

```{r}
#regression model
model_p3 <- lm(bwt ~ babysex + blength + fincome + smoken,
               data = birthweight)


#plot for model
birthweight %>%
  add_residuals(model_p3) %>%
  add_predictions(model_p3) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = 'red')
```
To create my model, I took into consideration which factors/variables I believed would be most influential to a baby's birthweight. Based on working on similar datasets in the past, I believed that the sex, length are great influential factor. Other things to greatly consider as the mothers behaviors, which can be influenced by money and where the mother smokes or not.


```{r}
#model 2
model2_p3 <- lm(bwt ~ blength + gaweeks,
                data = birthweight)

#model 3
model3_p3 <- lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex +
                  bhead*blength*babysex,
                data = birthweight)
```

```{r}
#creating training & testing data
cv_birthweight <-
  crossv_mc(birthweight, 100)
```

```{r}
cv_birthweight <- cv_birthweight %>%
  mutate(
    model_p3 = map(train, \(df) lm(bwt ~ babysex + blength + fincome + smoken, data = df)),
    model2_p3 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    model3_p3 = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex +
                  bhead*blength*babysex, data = df))) %>%
  mutate(
    
    rmse_model1 = map2_dbl(model_p3, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model2 = map2_dbl(model2_p3, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model3 = map2_dbl(model3_p3, test, \(mod, df) rmse(model = mod, data = df)))
```

```{r}
#plotting
cv_birthweight %>%
  select(starts_with('rmse')) %>%
  pivot_longer(
    everything(),
    names_to = 'model',
    values_to = 'rmse',
    names_prefix = 'rmse_') %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

Based on the plot above, we can see that my model did not do too well compared to the other two models. It had a large RMSE value. 